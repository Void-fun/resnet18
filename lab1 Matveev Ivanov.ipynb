{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = torchvision.datasets.CIFAR10('cifar', train=True, download=True,  transform=torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize((32, 32)),\n",
    "                torchvision.transforms.ToTensor(), \n",
    "                torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test = torchvision.datasets.CIFAR10('cifar',\n",
    "        train=False, download=True,\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((32, 32)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, identity_downsample = None, stride=1) -> None:\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.identity_downsample = identity_downsample\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        start_val = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.identity_downsample is not None:\n",
    "            start_val = self.identity_downsample(start_val)\n",
    "        x = x + start_val\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=1, padding=0)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "        self.layer1 = self.make_layer(64, 64, stride=1)\n",
    "        self.layer2 = self.make_layer(64, 128, stride=2)\n",
    "        self.layer3 = self.make_layer(128, 256, stride=2)\n",
    "        self.layer4 = self.make_layer(256, 512, stride=2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def make_layer(self, in_channels, out_channels, stride):\n",
    "        \n",
    "        identity_downsample = None\n",
    "        if stride != 1:\n",
    "            identity_downsample = self.identity_downsample(in_channels, out_channels)\n",
    "            \n",
    "        return nn.Sequential(\n",
    "            ResBlock(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride), \n",
    "            ResBlock(out_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def identity_downsample(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_curve(current_epoch):\n",
    "    #if device == f'cuda:{device_id}':\n",
    "    #    y_err = y_err.cpu()\n",
    "    x_epoch.append(current_epoch)\n",
    "    ax0.plot(x_epoch, y_loss['train'], 'bo-', label='train')\n",
    "    ax0.plot(x_epoch, y_loss['val'], 'ro-', label='val')\n",
    "    ax1.plot(x_epoch, y_err['train'], 'bo-', label='train')\n",
    "    ax1.plot(x_epoch, y_err['val'], 'ro-', label='val')\n",
    "    if current_epoch == 0:\n",
    "        ax0.legend()\n",
    "        ax1.legend()\n",
    "    fig.savefig('train.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.0659044981002808, acc: 0.4162674650698603, acc2: 0.4171\n",
      "epoch: 2, loss: 0.9433782696723938, acc: 0.5169660678642715, acc2: 0.518\n",
      "epoch: 3, loss: 0.8989146947860718, acc: 0.5849301397205589, acc2: 0.5861000000000001\n",
      "epoch: 4, loss: 0.7065101861953735, acc: 0.6732534930139721, acc2: 0.6746\n",
      "epoch: 5, loss: 0.528335690498352, acc: 0.6765469061876247, acc2: 0.6779000000000001\n",
      "epoch: 6, loss: 0.5139824748039246, acc: 0.6929141716566867, acc2: 0.6943\n",
      "epoch: 7, loss: 0.4710713326931, acc: 0.6846307385229541, acc2: 0.686\n",
      "epoch: 8, loss: 0.40332940220832825, acc: 0.687125748502994, acc2: 0.6885\n",
      "epoch: 9, loss: 0.43536773324012756, acc: 0.7220558882235529, acc2: 0.7235\n",
      "epoch: 10, loss: 0.2884400188922882, acc: 0.7131736526946107, acc2: 0.7146\n",
      "epoch: 11, loss: 0.3051035702228546, acc: 0.7074850299401197, acc2: 0.7089000000000001\n",
      "epoch: 12, loss: 0.20500773191452026, acc: 0.7137724550898203, acc2: 0.7152000000000001\n",
      "epoch: 13, loss: 0.1893100142478943, acc: 0.7141716566866267, acc2: 0.7156\n",
      "epoch: 14, loss: 0.1815996915102005, acc: 0.7396207584830339, acc2: 0.7411\n",
      "epoch: 15, loss: 0.20144176483154297, acc: 0.7335329341317365, acc2: 0.735\n",
      "epoch: 16, loss: 0.14360171556472778, acc: 0.7471057884231537, acc2: 0.7486\n",
      "epoch: 17, loss: 0.1107344999909401, acc: 0.6961077844311377, acc2: 0.6975\n",
      "epoch: 18, loss: 0.129062220454216, acc: 0.7473053892215569, acc2: 0.7488\n",
      "epoch: 19, loss: 0.09587220847606659, acc: 0.7087824351297405, acc2: 0.7102\n",
      "epoch: 20, loss: 0.13151033222675323, acc: 0.7550898203592814, acc2: 0.7566\n",
      "epoch: 21, loss: 0.09216444194316864, acc: 0.755688622754491, acc2: 0.7572\n",
      "epoch: 22, loss: 0.10931576788425446, acc: 0.7582834331337326, acc2: 0.7598\n",
      "epoch: 23, loss: 0.09215464442968369, acc: 0.7233532934131737, acc2: 0.7248\n",
      "epoch: 24, loss: 0.0722007304430008, acc: 0.7675648702594811, acc2: 0.7691\n",
      "epoch: 25, loss: 0.06926947832107544, acc: 0.7662674650698603, acc2: 0.7678\n",
      "epoch: 26, loss: 0.06598211824893951, acc: 0.7691616766467065, acc2: 0.7707\n",
      "epoch: 27, loss: 0.07064403593540192, acc: 0.7567864271457085, acc2: 0.7583000000000001\n",
      "epoch: 28, loss: 0.08025042712688446, acc: 0.7634730538922155, acc2: 0.765\n",
      "epoch: 29, loss: 0.06824073940515518, acc: 0.7823353293413173, acc2: 0.7839\n",
      "epoch: 30, loss: 0.07285868376493454, acc: 0.7792415169660679, acc2: 0.7808\n",
      "epoch: 31, loss: 0.03641883656382561, acc: 0.7542914171656686, acc2: 0.7558\n",
      "epoch: 32, loss: 0.09084588289260864, acc: 0.7603792415169661, acc2: 0.7619\n",
      "epoch: 33, loss: 0.06206364929676056, acc: 0.7434131736526947, acc2: 0.7449\n",
      "epoch: 34, loss: 0.08283629268407822, acc: 0.7399201596806387, acc2: 0.7414000000000001\n",
      "epoch: 35, loss: 0.059473611414432526, acc: 0.7529940119760479, acc2: 0.7545000000000001\n",
      "epoch: 36, loss: 0.041122786700725555, acc: 0.7585828343313373, acc2: 0.7601\n",
      "epoch: 37, loss: 0.04156147316098213, acc: 0.7502994011976047, acc2: 0.7518\n",
      "epoch: 38, loss: 0.0629442036151886, acc: 0.7826347305389222, acc2: 0.7842\n",
      "epoch: 39, loss: 0.04778493195772171, acc: 0.7590818363273453, acc2: 0.7606\n",
      "epoch: 40, loss: 0.03922045975923538, acc: 0.7533932135728543, acc2: 0.7549\n",
      "epoch: 41, loss: 0.03911573067307472, acc: 0.7796407185628742, acc2: 0.7812\n",
      "epoch: 42, loss: 0.04111809283494949, acc: 0.7657684630738523, acc2: 0.7673\n",
      "epoch: 43, loss: 0.053351424634456635, acc: 0.7554890219560878, acc2: 0.757\n",
      "epoch: 44, loss: 0.04511348158121109, acc: 0.7716566866267465, acc2: 0.7732\n",
      "epoch: 45, loss: 0.035600416362285614, acc: 0.7712574850299401, acc2: 0.7728\n",
      "epoch: 46, loss: 0.04656383395195007, acc: 0.7587824351297405, acc2: 0.7603000000000001\n",
      "epoch: 47, loss: 0.03680764511227608, acc: 0.7799401197604791, acc2: 0.7815000000000001\n",
      "epoch: 48, loss: 0.025599192827939987, acc: 0.786626746506986, acc2: 0.7882\n",
      "epoch: 49, loss: 0.05679132789373398, acc: 0.7377245508982035, acc2: 0.7392000000000001\n",
      "epoch: 50, loss: 0.027453476563096046, acc: 0.7577844311377245, acc2: 0.7593000000000001\n",
      "epoch: 51, loss: 0.04353107884526253, acc: 0.7739520958083832, acc2: 0.7755000000000001\n",
      "epoch: 52, loss: 0.061134129762649536, acc: 0.7759481037924152, acc2: 0.7775000000000001\n",
      "epoch: 53, loss: 0.028316861018538475, acc: 0.7646706586826347, acc2: 0.7662\n",
      "epoch: 54, loss: 0.06557147204875946, acc: 0.7655688622754491, acc2: 0.7671\n",
      "epoch: 55, loss: 0.05011012405157089, acc: 0.774750499001996, acc2: 0.7763\n",
      "epoch: 56, loss: 0.027675407007336617, acc: 0.7853293413173653, acc2: 0.7869\n",
      "epoch: 57, loss: 0.05920722335577011, acc: 0.7774451097804391, acc2: 0.779\n",
      "epoch: 58, loss: 0.06213180720806122, acc: 0.7565868263473053, acc2: 0.7581\n",
      "epoch: 59, loss: 0.06930418312549591, acc: 0.768562874251497, acc2: 0.7701\n",
      "epoch: 60, loss: 0.01924152858555317, acc: 0.7681636726546907, acc2: 0.7697\n",
      "epoch: 61, loss: 0.033701784908771515, acc: 0.775748502994012, acc2: 0.7773\n",
      "epoch: 62, loss: 0.040965158492326736, acc: 0.761377245508982, acc2: 0.7629\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-05.\n",
      "epoch: 63, loss: 0.052879396826028824, acc: 0.7667664670658683, acc2: 0.7683\n",
      "epoch: 64, loss: 0.00936389435082674, acc: 0.7918163672654691, acc2: 0.7934\n",
      "epoch: 65, loss: 0.01041414588689804, acc: 0.7913173652694611, acc2: 0.7929\n",
      "epoch: 66, loss: 0.024047544226050377, acc: 0.7947105788423153, acc2: 0.7963\n",
      "epoch: 67, loss: 0.013732030056416988, acc: 0.7993013972055888, acc2: 0.8009000000000001\n",
      "epoch: 68, loss: 0.013041510246694088, acc: 0.7963073852295409, acc2: 0.7979\n",
      "epoch: 69, loss: 0.002842796267941594, acc: 0.7977045908183633, acc2: 0.7993\n",
      "epoch: 70, loss: 0.00235165748745203, acc: 0.7992015968063872, acc2: 0.8008000000000001\n",
      "epoch: 71, loss: 0.006188621744513512, acc: 0.7961077844311377, acc2: 0.7977000000000001\n",
      "epoch: 72, loss: 0.0019137663766741753, acc: 0.8025948103792415, acc2: 0.8042\n",
      "epoch: 73, loss: 0.0035942879039794207, acc: 0.7993013972055888, acc2: 0.8009000000000001\n",
      "epoch: 74, loss: 0.004756288602948189, acc: 0.8017964071856287, acc2: 0.8034\n",
      "epoch: 75, loss: 0.003267925465479493, acc: 0.7975049900199601, acc2: 0.7991\n",
      "epoch: 76, loss: 0.011401832103729248, acc: 0.7991017964071856, acc2: 0.8007000000000001\n",
      "epoch: 77, loss: 0.0010609213495627046, acc: 0.8029940119760479, acc2: 0.8046000000000001\n",
      "epoch: 78, loss: 0.0016269935294985771, acc: 0.800499001996008, acc2: 0.8021\n",
      "epoch: 79, loss: 0.007418582681566477, acc: 0.7998003992015968, acc2: 0.8014\n",
      "epoch: 80, loss: 0.01055683009326458, acc: 0.8029940119760479, acc2: 0.8046000000000001\n",
      "epoch: 81, loss: 0.00219396548345685, acc: 0.7982035928143713, acc2: 0.7998000000000001\n",
      "epoch: 82, loss: 0.0033133903052657843, acc: 0.800499001996008, acc2: 0.8021\n",
      "epoch: 83, loss: 0.001519175712019205, acc: 0.8041916167664671, acc2: 0.8058000000000001\n",
      "epoch: 84, loss: 0.0018227873370051384, acc: 0.8028942115768463, acc2: 0.8045\n",
      "epoch: 85, loss: 0.0027175135910511017, acc: 0.8020958083832336, acc2: 0.8037000000000001\n",
      "epoch: 86, loss: 0.0014073685742914677, acc: 0.8005988023952095, acc2: 0.8022\n",
      "epoch: 87, loss: 0.005905548110604286, acc: 0.8018962075848304, acc2: 0.8035\n",
      "epoch: 88, loss: 0.0008864930714480579, acc: 0.7968063872255489, acc2: 0.7984\n",
      "epoch: 89, loss: 0.0015091202221810818, acc: 0.7975049900199601, acc2: 0.7991\n",
      "epoch: 90, loss: 0.0009148324024863541, acc: 0.7989021956087824, acc2: 0.8005\n",
      "epoch: 91, loss: 0.006011354271322489, acc: 0.7974051896207585, acc2: 0.799\n",
      "epoch: 92, loss: 0.002532775979489088, acc: 0.7925149700598803, acc2: 0.7941\n",
      "epoch: 93, loss: 0.0017428178107365966, acc: 0.7960079840319362, acc2: 0.7976000000000001\n",
      "epoch: 94, loss: 0.0008318189065903425, acc: 0.7946107784431138, acc2: 0.7962\n",
      "epoch: 95, loss: 0.002304786117747426, acc: 0.7939121756487026, acc2: 0.7955\n",
      "epoch: 96, loss: 0.005275128409266472, acc: 0.8035928143712575, acc2: 0.8052\n",
      "epoch: 97, loss: 0.000808281940408051, acc: 0.800499001996008, acc2: 0.8021\n",
      "epoch: 98, loss: 0.0010551802115514874, acc: 0.8021956087824351, acc2: 0.8038000000000001\n",
      "epoch: 99, loss: 0.004202918615192175, acc: 0.7934131736526946, acc2: 0.795\n",
      "epoch: 100, loss: 0.0006971445982344449, acc: 0.7960079840319362, acc2: 0.7976000000000001\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-06.\n",
      "epoch: 101, loss: 0.001041167532093823, acc: 0.7920159680638723, acc2: 0.7936000000000001\n",
      "epoch: 102, loss: 0.017423834651708603, acc: 0.7983033932135729, acc2: 0.7999\n",
      "epoch: 103, loss: 0.000582047738134861, acc: 0.7969061876247505, acc2: 0.7985\n",
      "epoch: 104, loss: 0.0024160011671483517, acc: 0.7976047904191617, acc2: 0.7992\n",
      "epoch: 105, loss: 0.0009854265954345465, acc: 0.7970059880239521, acc2: 0.7986000000000001\n",
      "epoch: 106, loss: 0.0003300629905425012, acc: 0.7963073852295409, acc2: 0.7979\n",
      "epoch: 107, loss: 0.0005802170489914715, acc: 0.7941117764471058, acc2: 0.7957000000000001\n",
      "epoch: 108, loss: 0.002145384205505252, acc: 0.7944111776447106, acc2: 0.796\n",
      "epoch: 109, loss: 0.00037940582842566073, acc: 0.7984031936127745, acc2: 0.8\n",
      "epoch: 110, loss: 0.000524462666362524, acc: 0.7974051896207585, acc2: 0.799\n",
      "epoch: 111, loss: 0.0022099395282566547, acc: 0.7978043912175649, acc2: 0.7994\n",
      "epoch: 112, loss: 0.00033761016675271094, acc: 0.7976047904191617, acc2: 0.7992\n",
      "epoch: 113, loss: 0.0013491816353052855, acc: 0.7967065868263473, acc2: 0.7983\n",
      "epoch: 114, loss: 0.0010396422585472465, acc: 0.7982035928143713, acc2: 0.7998000000000001\n",
      "epoch: 115, loss: 0.0007145967683754861, acc: 0.7999001996007984, acc2: 0.8015\n",
      "epoch: 116, loss: 0.001795284217223525, acc: 0.7963073852295409, acc2: 0.7979\n",
      "epoch: 117, loss: 0.004695598501712084, acc: 0.7981037924151697, acc2: 0.7997000000000001\n",
      "Epoch 00118: reducing learning rate of group 0 to 1.0000e-07.\n",
      "epoch: 118, loss: 0.0004126403364352882, acc: 0.7991017964071856, acc2: 0.8007000000000001\n",
      "epoch: 119, loss: 0.00047321911551989615, acc: 0.7993013972055888, acc2: 0.8009000000000001\n",
      "epoch: 120, loss: 0.00034152294392697513, acc: 0.8003992015968064, acc2: 0.802\n",
      "epoch: 121, loss: 0.0003250606241635978, acc: 0.7961077844311377, acc2: 0.7977000000000001\n",
      "epoch: 122, loss: 0.0008761753560975194, acc: 0.7988023952095809, acc2: 0.8004\n",
      "epoch: 123, loss: 0.00035188213223591447, acc: 0.7999001996007984, acc2: 0.8015\n",
      "epoch: 124, loss: 0.0009362396085634828, acc: 0.799001996007984, acc2: 0.8006000000000001\n",
      "epoch: 125, loss: 0.000368620763765648, acc: 0.8005988023952095, acc2: 0.8022\n",
      "epoch: 126, loss: 0.00027771451277658343, acc: 0.7982035928143713, acc2: 0.7998000000000001\n",
      "epoch: 127, loss: 0.0016080515924841166, acc: 0.8023952095808383, acc2: 0.804\n",
      "epoch: 128, loss: 0.00034991721622645855, acc: 0.8011976047904191, acc2: 0.8028000000000001\n",
      "epoch: 129, loss: 0.0016434649005532265, acc: 0.7977045908183633, acc2: 0.7993\n",
      "epoch: 130, loss: 0.0006198417395353317, acc: 0.798502994011976, acc2: 0.8001\n",
      "epoch: 131, loss: 0.0007411513943225145, acc: 0.7977045908183633, acc2: 0.7993\n",
      "epoch: 132, loss: 0.00042250793194398284, acc: 0.800499001996008, acc2: 0.8021\n",
      "epoch: 133, loss: 0.0003894889960065484, acc: 0.7987025948103792, acc2: 0.8003\n",
      "epoch: 134, loss: 0.00033778868964873254, acc: 0.8002994011976048, acc2: 0.8019000000000001\n",
      "epoch: 135, loss: 0.00038920139195397496, acc: 0.798502994011976, acc2: 0.8001\n",
      "epoch: 136, loss: 0.001156963873654604, acc: 0.7984031936127745, acc2: 0.8\n",
      "epoch: 137, loss: 0.000846425595227629, acc: 0.8000998003992016, acc2: 0.8017000000000001\n",
      "epoch: 138, loss: 0.00041998422238975763, acc: 0.7982035928143713, acc2: 0.7998000000000001\n",
      "epoch: 139, loss: 0.0010524507379159331, acc: 0.7999001996007984, acc2: 0.8015\n",
      "epoch: 140, loss: 0.0004963657702319324, acc: 0.8003992015968064, acc2: 0.802\n",
      "epoch: 141, loss: 0.0004767435893882066, acc: 0.7996007984031936, acc2: 0.8012\n",
      "epoch: 142, loss: 0.00026702243485488, acc: 0.8026946107784431, acc2: 0.8043\n",
      "epoch: 143, loss: 0.00029072794131934643, acc: 0.801497005988024, acc2: 0.8031\n",
      "Epoch 00144: reducing learning rate of group 0 to 1.0000e-08.\n",
      "epoch: 144, loss: 0.0007035162998363376, acc: 0.7986027944111777, acc2: 0.8002\n",
      "epoch: 145, loss: 0.0005366378463804722, acc: 0.8008982035928144, acc2: 0.8025\n",
      "epoch: 146, loss: 0.0004753343528136611, acc: 0.8015968063872255, acc2: 0.8032\n",
      "epoch: 147, loss: 0.0010258478578180075, acc: 0.8015968063872255, acc2: 0.8032\n",
      "epoch: 148, loss: 0.00259422161616385, acc: 0.7953093812375249, acc2: 0.7969\n",
      "epoch: 149, loss: 0.0005703752394765615, acc: 0.8, acc2: 0.8016000000000001\n",
      "epoch: 150, loss: 0.0003633365558926016, acc: 0.7994011976047904, acc2: 0.801\n",
      "epoch: 151, loss: 0.001831839676015079, acc: 0.8005988023952095, acc2: 0.8022\n",
      "epoch: 152, loss: 0.0011922448175027966, acc: 0.8015968063872255, acc2: 0.8032\n",
      "epoch: 153, loss: 0.0014623922761529684, acc: 0.8013972055888223, acc2: 0.803\n",
      "epoch: 154, loss: 0.00042408087756484747, acc: 0.7986027944111777, acc2: 0.8002\n",
      "epoch: 155, loss: 0.00020743298227898777, acc: 0.8009980039920159, acc2: 0.8026000000000001\n",
      "epoch: 156, loss: 0.0003823156876023859, acc: 0.8031936127744511, acc2: 0.8048000000000001\n",
      "epoch: 157, loss: 0.0004661433049477637, acc: 0.799001996007984, acc2: 0.8006000000000001\n",
      "epoch: 158, loss: 0.0004370459064375609, acc: 0.8001996007984032, acc2: 0.8018000000000001\n",
      "epoch: 159, loss: 0.00017926313739735633, acc: 0.8007984031936127, acc2: 0.8024\n",
      "epoch: 160, loss: 0.0007987362332642078, acc: 0.7987025948103792, acc2: 0.8003\n",
      "epoch: 161, loss: 0.00036049942718818784, acc: 0.7996007984031936, acc2: 0.8012\n",
      "epoch: 162, loss: 0.0009150739060714841, acc: 0.80249500998004, acc2: 0.8041\n",
      "epoch: 163, loss: 0.00046897545689716935, acc: 0.8003992015968064, acc2: 0.802\n",
      "epoch: 164, loss: 0.0007150162709876895, acc: 0.8019960079840319, acc2: 0.8036000000000001\n",
      "epoch: 165, loss: 0.0002040589606622234, acc: 0.7988023952095809, acc2: 0.8004\n",
      "epoch: 166, loss: 0.0002874524798244238, acc: 0.800499001996008, acc2: 0.8021\n",
      "epoch: 167, loss: 0.00033127426286228, acc: 0.8002994011976048, acc2: 0.8019000000000001\n",
      "epoch: 168, loss: 0.0006044431356713176, acc: 0.8016966067864272, acc2: 0.8033\n",
      "epoch: 169, loss: 0.0009767916053533554, acc: 0.8013972055888223, acc2: 0.803\n",
      "epoch: 170, loss: 0.0004537434142548591, acc: 0.7996007984031936, acc2: 0.8012\n",
      "epoch: 171, loss: 0.0004271574434824288, acc: 0.8, acc2: 0.8016000000000001\n",
      "epoch: 172, loss: 0.0003356629458721727, acc: 0.8019960079840319, acc2: 0.8036000000000001\n",
      "epoch: 173, loss: 0.000251251389272511, acc: 0.7981037924151697, acc2: 0.7997000000000001\n",
      "epoch: 174, loss: 0.0003358328540343791, acc: 0.8002994011976048, acc2: 0.8019000000000001\n",
      "epoch: 175, loss: 0.0005645225755870342, acc: 0.801497005988024, acc2: 0.8031\n",
      "epoch: 176, loss: 0.0004036810714751482, acc: 0.8009980039920159, acc2: 0.8026000000000001\n",
      "epoch: 177, loss: 0.0018774354830384254, acc: 0.80249500998004, acc2: 0.8041\n",
      "epoch: 178, loss: 0.003278598887845874, acc: 0.7992015968063872, acc2: 0.8008000000000001\n",
      "epoch: 179, loss: 0.0007694653468206525, acc: 0.7980039920159681, acc2: 0.7996000000000001\n",
      "epoch: 180, loss: 0.00041684837196953595, acc: 0.8012974051896208, acc2: 0.8029000000000001\n",
      "epoch: 181, loss: 0.00034970729029737413, acc: 0.80249500998004, acc2: 0.8041\n",
      "epoch: 182, loss: 0.001149575226008892, acc: 0.8012974051896208, acc2: 0.8029000000000001\n",
      "epoch: 183, loss: 0.00036629632813856006, acc: 0.8003992015968064, acc2: 0.802\n",
      "epoch: 184, loss: 0.00038097990909591317, acc: 0.7991017964071856, acc2: 0.8007000000000001\n",
      "epoch: 185, loss: 0.0010337280109524727, acc: 0.7992015968063872, acc2: 0.8008000000000001\n",
      "epoch: 186, loss: 0.00038272759411484003, acc: 0.7986027944111777, acc2: 0.8002\n",
      "epoch: 187, loss: 0.006603551562875509, acc: 0.8007984031936127, acc2: 0.8024\n",
      "epoch: 188, loss: 0.002108913380652666, acc: 0.798502994011976, acc2: 0.8001\n",
      "epoch: 189, loss: 0.00035119912354275584, acc: 0.7999001996007984, acc2: 0.8015\n",
      "epoch: 190, loss: 0.003383805975317955, acc: 0.8006986027944112, acc2: 0.8023\n",
      "epoch: 191, loss: 0.00027454638620838523, acc: 0.7992015968063872, acc2: 0.8008000000000001\n",
      "epoch: 192, loss: 0.0004207184538245201, acc: 0.799001996007984, acc2: 0.8006000000000001\n",
      "epoch: 193, loss: 0.0022256961092352867, acc: 0.8021956087824351, acc2: 0.8038000000000001\n",
      "epoch: 194, loss: 0.00033166760113090277, acc: 0.8005988023952095, acc2: 0.8022\n",
      "epoch: 195, loss: 0.00038739139563404024, acc: 0.7975049900199601, acc2: 0.7991\n",
      "epoch: 196, loss: 0.00018724116671364754, acc: 0.7997005988023952, acc2: 0.8013\n",
      "epoch: 197, loss: 0.00040153725421987474, acc: 0.7989021956087824, acc2: 0.8005\n",
      "epoch: 198, loss: 0.0004668008768931031, acc: 0.8017964071856287, acc2: 0.8034\n",
      "epoch: 199, loss: 0.002160173375159502, acc: 0.8009980039920159, acc2: 0.8026000000000001\n",
      "epoch: 200, loss: 0.000532749283593148, acc: 0.8006986027944112, acc2: 0.8023\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ResNet()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3) # 0,001\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=5, verbose=True)\n",
    "\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "x_epoch = []\n",
    "y_loss = {}  # loss history\n",
    "y_loss['train'] = []\n",
    "y_loss['val'] = []\n",
    "y_err = {}\n",
    "y_err['train'] = []\n",
    "y_err['val'] = []\n",
    "\n",
    "fig = plt.figure()\n",
    "ax0 = fig.add_subplot(121, title=\"loss\")\n",
    "ax1 = fig.add_subplot(122, title=\"top1err\")\n",
    "\n",
    "for epoch in range(200):\n",
    "  # optimizer = optim_scheduler(epoch)\n",
    "  running_loss = 0\n",
    "  running_corrects = 0\n",
    "  for idx, (x, label) in enumerate(train):\n",
    "    x, label = x.to(device), label.to(device)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, label)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item() * x.size(0)\n",
    "    running_corrects += torch.sum(preds == label.data)\n",
    "  \n",
    "  train_loss = running_loss / len(train.dataset)\n",
    "  # print(train_loss)\n",
    "  epoch_acc = (running_corrects.double() / len(train.dataset)).cpu()\n",
    "  y_loss['train'].append(train_loss)\n",
    "  y_err['train'].append(1.0 - epoch_acc)\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    lr_scheduler.step(train_loss)\n",
    "    tot_corr = 0\n",
    "    tot_num = len(test)\n",
    "      \n",
    "    # epoch_acc = running_corrects.double() / len(test.dataset)\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for x, label in test:\n",
    "      x, label = x.to(device), label.to(device)\n",
    "      outputs = model(x)\n",
    "      pred = outputs.argmax(dim=1)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "      \n",
    "      # loss.backward()\n",
    "\n",
    "      running_loss += loss.item() * x.size(0)\n",
    "      running_corrects += torch.sum(preds == label.data)\n",
    "      tot_corr += torch.eq(pred, label).double().sum().item() # using item() to convert tensor to number\n",
    "      tot_num += x.size(0)\n",
    "    epoch_acc = (running_corrects.double() / len(test.dataset)).cpu()\n",
    "    acc = tot_corr / tot_num\n",
    "  test_loss = running_loss / len(test.dataset)\n",
    "  loss_list.append(loss.item())\n",
    "  acc_list.append(acc)\n",
    "  # print(test_loss)\n",
    "  y_loss['val'].append(test_loss)\n",
    "  y_err['val'].append(1.0 - epoch_acc)\n",
    "  draw_curve(epoch)\n",
    "    \n",
    "  print(f'epoch: {epoch+1}, loss: {loss}, acc: {acc}, acc2: {epoch_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = ResNet()\n",
    "model.to(device)\n",
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import time\n",
    "import copy\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=50, is_inception=False):\n",
    "    \n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']: # Each epoch has a training and validation phase\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]: # Iterate over data\n",
    "                \n",
    "                # inputs = torch.transforms.functional.resize(inputs, (112, 112))\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad() # Zero the parameter gradients\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'): # Forward. Track history if only in train\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train': # Backward + optimize only if in training phase\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if phase == 'val': # Adjust learning rate based on val loss\n",
    "                lr_scheduler.step(epoch_loss)\n",
    "                \n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model, _ = train_model(model, {\"train\": train, \"val\": test}, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdae097ff6608b61c57ad9a2af23b6f90ce9d579b2e56b2697630202e9ca4d90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
